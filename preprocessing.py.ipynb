{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: STORE SENTIMENT INFO IN A WAY THAT ALLOWS US TO CREATE RUNNING SENTIMENT GRAPHS\n",
    "\n",
    "- Cut text into 20 or so roughly equal sized chunks of sentences\n",
    "- Store mean positive and negative sentiment values for each segment as a feature \n",
    "- Create graphs of sentiment movement for data viz (upper bound == max value observed, lower bound == 0)\n",
    "\n",
    "\"\"\"\n",
    "#need this or else it throws encoding/decoding errors\n",
    "reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "punct = set(['!', '#', '\"', '%', '$', '&', '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', '<', '?', '>', '@', '[', ']', '_', '^', '`', '{', '~'])\n",
    "table = str.maketrans(\"\",\"\")\n",
    "target = open(\"output_POS.csv\", 'w')\n",
    "\n",
    "#check avg sent size\n",
    "target.write(\"book_name|total_words|avg_sentence_size|!|#|''|%|$|&|(|+|*|-|,|/|.|;|:|=|<|?|>|@|[|]|_|^|`|{|~|neg|neu|pos|compound|ID|Title|Author|CC|CD|DT|EX|FW|IN|JJ|JJR|JJS|LS|MD|NN|NNP|NNPS|NNS|PDT|PRP|PRP$|RB|RBR|RBS|RP|VB|VBD|VBG|VBP|VBN|WDT|VBZ|WRB|WP$|WP|\")\n",
    "target.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctAndWordsInSentence(listOfCharacters):\n",
    "    punctuation_dict = {}\n",
    "    sentenceCounter = 0;\n",
    "    wordCounter = 0\n",
    "    periodCounter = 0.1;\n",
    "    avgSentenceSize = 0;\n",
    "    totalWords = 0;\n",
    "    punctCounter = 0;\n",
    "    \"\"\"\n",
    "    Iterate through all characters. Count periods, punct frequencies. WordCounter = words in sentence (resets\n",
    "    to zero after a period). totalWords is the book's total word count.\n",
    "    \"\"\"\n",
    "    #sentence count\n",
    "    for i in range(len(listOfCharacters)):\n",
    "        if i != 0:\n",
    "            #if lettter followed by space or punct, then word count +=1\n",
    "            if (listOfCharacters[i] == \" \" or str(listOfCharacters[i]) in punct) and str(listOfCharacters[i-1]) in (string.ascii_lowercase + string.ascii_uppercase):\n",
    "                totalWords = totalWords + 1\n",
    "            #count periods as well\n",
    "            if listOfCharacters[i] == \".\":\n",
    "                periodCounter = periodCounter + 1\n",
    "            if listOfCharacters[i] in punct:\n",
    "                punctCounter = punctCounter + 1\n",
    "                if listOfCharacters[i] in punctuation_dict:\n",
    "                    punctuation_dict[listOfCharacters[i]] = punctuation_dict[listOfCharacters[i]] + 1\n",
    "                else:\n",
    "                    punctuation_dict[listOfCharacters[i]] = 1\n",
    "\n",
    "\n",
    "\n",
    "    avgSentenceSize = (totalWords/periodCounter)\n",
    "    #put together output, bar delimited\n",
    "    target.write(str(totalWords) + \"|\")\n",
    "    target.write(str(avgSentenceSize) + \"|\")\n",
    "    \n",
    "    for i in punct:\n",
    "        s = \"\"\n",
    "        if i in punctuation_dict:\n",
    "            s = s + str(punctuation_dict[i] / punctCounter) + \"|\"    #pct of punct that is [x]\n",
    "        else:\n",
    "            s = s + str(0) + \"|\"                                     #0 if unused\n",
    "        target.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(content):\n",
    "    parts = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"VB\", \"VBD\", \"VBG\",  \"VBP\", \"VBN\", \"WDT\", \"VBZ\", \"WRB\", \"WP$\", \"WP\" ]\n",
    "    content = str(content)   #see if this fixes the error\n",
    "    text = nltk.word_tokenize(content)  #need to tokenize first\n",
    "    results = nltk.pos_tag(text)\n",
    "    #dict of {POS: count}\n",
    "    results_dict = {}\n",
    "    counter = 0\n",
    "    for tag in results:\n",
    "        token = tag[0]\n",
    "        pos = tag[1]\n",
    "        counter += 1\n",
    "        if pos in results_dict:\n",
    "            results_dict[pos] += 1\n",
    "        else:\n",
    "            results_dict[pos] = 1\n",
    "    #write to file\n",
    "    for i in parts:\n",
    "        s = \"\"\n",
    "        if i in results_dict:\n",
    "            s = s + str(results_dict[i]/float(counter)) + \"|\"    #pct of POS that are [x]\n",
    "        else:\n",
    "            s = s + str(0) + \"|\"                                 #0 if unused\n",
    "        target.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_file(text):\n",
    "    return text.decode('utf-8', errors='replace')\n",
    "\n",
    "def get_title_author(text):\n",
    "    author = \"NULL\"\n",
    "    title = \"NULL\"\n",
    "    text = text.splitlines()\n",
    "    #for line in text, check if title or author stored there\n",
    "    for i in range(80):\n",
    "        #error handling since some texts are <80 lines\n",
    "        try:\n",
    "            if \"Title: \" in text[i]:\n",
    "                title = text[i][7:]\n",
    "            if \"Author: \" in text[i]:\n",
    "                author = text[i][8:]\n",
    "            #if they have both been found, do not waste extra time iterating \n",
    "            if title != \"NULL\" and author != \"NULL\":\n",
    "                title_author_tuple = (title, author)\n",
    "                return title_author_tuple\n",
    "        except:\n",
    "            pass\n",
    "    title_author_tuple = (title, author)\n",
    "    return title_author_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(temp):\n",
    "    temp = b\"temp.replace('\\n', '')\"\n",
    "    temp = b\"temp.replace('\\r', '')\"\n",
    "    content = decode_file(temp)\n",
    "    content = tokenize.sent_tokenize(content)\n",
    "    #get author and title now that content is split by sentence \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    booksent = []\n",
    "    for sentence in content:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        ssarray = [ss['neg'],ss['neu'],ss['pos'], ss['compound']]\n",
    "        booksent.append(ssarray)\n",
    "    valuearray = np.array(booksent)\n",
    "    # mean negative, neutral, positive, compound score for all lines in book\n",
    "    values = np.mean(valuearray, axis=0)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    '''\n",
    "    read file as a list of words\n",
    "    set lowercase, stem, remove stopwords\n",
    "    get punctuation string for later feature extraction\n",
    "    save local wordcount dict\n",
    "    save global word dict after finished looping through docs\n",
    "    '''\n",
    "    counter = 0\n",
    "    cnt=0\n",
    "    for book in os.listdir(\"C:/Users/krgau/OneDrive/Desktop/data/new/new1/\"):\n",
    "        if cnt<2500:\n",
    "            if not book.startswith('.'):    #pass hidden files such as .DS_STORE\n",
    "                book = str(book)\n",
    "                with open(\"C:/Users/krgau/OneDrive/Desktop/data/new/new1/\" + book, 'rb') as f:\n",
    "                    content =b\"f.read().rstrip('\\n')\"\n",
    "                target.write(book + \"|\")\n",
    "                punctAndWordsInSentence(content)\n",
    "                sentiment_values = get_sentiment(content)\n",
    "                neg = sentiment_values[0]\n",
    "                neu = sentiment_values[1]\n",
    "                pos = sentiment_values[2]\n",
    "                compound = sentiment_values[3]\n",
    "                target.write(str(neg) + \"|\" + str(neu) + \"|\" + str(pos) + \"|\" + str(compound) + \"|\" + str(counter) + \"|\")\n",
    "                title_author_tuple = get_title_author(content)\n",
    "                target.write(str(title_author_tuple[0]) + \"|\" + str(title_author_tuple[1]) + \"|\")\n",
    "                pos_tagging(content)\n",
    "                target.write('\\n')\n",
    "                f.close()\n",
    "                counter += 1\n",
    "                cnt=cnt+1\n",
    "                if counter%20 == 0:\n",
    "                    print (\"book \" + str(counter) + \" done: \" + book)\n",
    "        else:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book 20 done: 10014-8.txt\n",
      "book 40 done: 10026-8.txt\n",
      "book 60 done: 10037-8.txt\n",
      "book 80 done: 10051.txt\n",
      "book 100 done: 10070.txt\n",
      "book 120 done: 10082.txt\n",
      "book 140 done: 10104-8.txt\n",
      "book 160 done: 10135-8.txt\n",
      "book 180 done: 10151.txt\n",
      "book 200 done: 10318-8.txt\n",
      "book 220 done: 10356.txt\n",
      "book 240 done: 10377.txt\n",
      "book 260 done: 10404.txt\n",
      "book 280 done: 10436.txt\n",
      "book 300 done: 10448.txt\n",
      "book 320 done: 10471-8.txt\n",
      "book 340 done: 10494.txt\n",
      "book 360 done: 10515.txt\n",
      "book 380 done: 10535.txt\n",
      "book 400 done: 10573.txt\n",
      "book 420 done: 10594.txt\n",
      "book 440 done: 10624-8.txt\n",
      "book 460 done: 10647-8.txt\n",
      "book 480 done: 10676-8.txt\n",
      "book 500 done: 10711.txt\n",
      "book 520 done: 10731.txt\n",
      "book 540 done: 10754.txt\n",
      "book 560 done: 10806-8.txt\n",
      "book 580 done: 10845-8.txt\n",
      "book 600 done: 10886.txt\n",
      "book 620 done: 10911-8.txt\n",
      "book 640 done: 10925-8.txt\n",
      "book 660 done: 10938.txt\n",
      "book 680 done: 10949.txt\n",
      "book 700 done: 10961.txt\n",
      "book 720 done: 10974-8.txt\n",
      "book 740 done: 10988.txt\n",
      "book 760 done: 11008.txt\n",
      "book 780 done: 11023.txt\n",
      "book 800 done: 11045-8.txt\n",
      "book 820 done: 11062-8.txt\n",
      "book 840 done: 11079.txt\n",
      "book 860 done: 11094-8.txt\n",
      "book 880 done: 11106.txt\n",
      "book 900 done: 11119.txt\n",
      "book 920 done: 11134.txt\n",
      "book 940 done: 11151.txt\n",
      "book 960 done: 11169-8.txt\n",
      "book 980 done: 11186.txt\n",
      "book 1000 done: 11201.txt\n",
      "book 1020 done: 11219.txt\n",
      "book 1040 done: 11231.txt\n",
      "book 1060 done: 11243.txt\n",
      "book 1080 done: 11255-8.txt\n",
      "book 1100 done: 11268.txt\n",
      "book 1120 done: 11280-8.txt\n",
      "book 1140 done: 11309-8.txt\n",
      "book 1160 done: 11324.txt\n",
      "book 1180 done: 11336.txt\n",
      "book 1200 done: 11350-8.txt\n",
      "book 1220 done: 11363-8.txt\n",
      "book 1240 done: 11376-8.txt\n",
      "book 1260 done: 11390.txt\n",
      "book 1280 done: 11403.txt\n",
      "book 1300 done: 11416-8.txt\n",
      "book 1320 done: 11428-8.txt\n",
      "book 1340 done: 11442-8.txt\n",
      "book 1360 done: 11454.txt\n",
      "book 1380 done: 11465-8.txt\n",
      "book 1400 done: 11485.txt\n",
      "book 1420 done: 11499.txt\n",
      "book 1440 done: 11514.txt\n",
      "book 1460 done: 11526.txt\n",
      "book 1480 done: 11539-8.txt\n",
      "book 1500 done: 11550.txt\n",
      "book 1520 done: 11562-8.txt\n",
      "book 1540 done: 11573-8.txt\n",
      "book 1560 done: 11585.txt\n",
      "book 1580 done: 11606.txt\n",
      "book 1600 done: 11617-8.txt\n",
      "book 1620 done: 11630-8.txt\n",
      "book 1640 done: 11642-8.txt\n",
      "book 1660 done: 11658.txt\n",
      "book 1680 done: 11674-8.txt\n",
      "book 1700 done: 11690.txt\n",
      "book 1720 done: 11701-8.txt\n",
      "book 1740 done: 11711.txt\n",
      "book 1760 done: 11725-8.txt\n",
      "book 1780 done: 11735.txt\n",
      "book 1800 done: 11748.txt\n",
      "book 1820 done: 11763.txt\n",
      "book 1840 done: 11808-8.txt\n",
      "book 1860 done: 11832-8.txt\n",
      "book 1880 done: 11852-8.txt\n",
      "book 1900 done: 11867-8.txt\n",
      "book 1920 done: 11877.txt\n",
      "book 1940 done: 11892.txt\n",
      "book 1960 done: 11906.txt\n",
      "book 1980 done: 11920.txt\n",
      "book 2000 done: 11935-0.txt\n",
      "book 2020 done: 11947-8.txt\n",
      "book 2040 done: 11960-8.txt\n",
      "book 2060 done: 11978.txt\n",
      "book 2080 done: 11989.txt\n",
      "book 2100 done: 12013-8.txt\n",
      "book 2120 done: 12029-8.txt\n",
      "book 2140 done: 12041.txt\n",
      "book 2160 done: 12054-8.txt\n",
      "book 2180 done: 12066.txt\n",
      "book 2200 done: 12081.txt\n",
      "book 2220 done: 12094-8.txt\n",
      "book 2240 done: 12107-8.txt\n",
      "book 2260 done: 12129.txt\n",
      "book 2280 done: 12144-8.txt\n",
      "book 2300 done: 12162.txt\n",
      "book 2320 done: 12179-8.txt\n",
      "book 2340 done: 12191-8.txt\n",
      "book 2360 done: 12203.txt\n",
      "book 2380 done: 12220.txt\n",
      "book 2400 done: 12233.txt\n",
      "book 2420 done: 12245.txt\n",
      "book 2440 done: 12262.txt\n",
      "book 2460 done: 12281.txt\n",
      "book 2480 done: 12294-8.txt\n",
      "book 2500 done: 12306.txt\n",
      "0:00:23.275852\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "preprocessing()\n",
    "print (datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
